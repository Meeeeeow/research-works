{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuFz_aXi3htx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n"
      ],
      "metadata": {
        "id": "z-_8hk1t4ERg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "0KRDIg6G5oBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Varying hyperparameters\n",
        "batch_sizes = [8, 16, 32]\n",
        "num_epochs_list = [1, 5, 10, 20]\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n"
      ],
      "metadata": {
        "id": "R-gh_tBp6I8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n"
      ],
      "metadata": {
        "id": "klVPHrI_6L2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_size in batch_sizes:\n",
        "    for num_epochs in num_epochs_list:\n",
        "        for learning_rate in learning_rates:\n",
        "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            model = MLP()\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "            for epoch in range(num_epochs):\n",
        "                model.train()\n",
        "                for images, labels in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(images)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in test_loader:\n",
        "                    outputs = model(images)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()\n",
        "\n",
        "            accuracy = correct / total\n",
        "\n",
        "            print(f'Batch Size: {batch_size}, Epochs: {num_epochs}, Learning Rate: {learning_rate}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_hyperparameters = {'Batch Size': batch_size, 'Epochs': num_epochs, 'Learning Rate': learning_rate}\n",
        "\n",
        "print(\"\\nBest Performing Hyperparameters:\")\n",
        "print(best_hyperparameters)\n",
        "print(\"Best Accuracy:\", best_accuracy * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz07ZqZw6R_K",
        "outputId": "0bed1f0c-c19c-40cf-9166-519cdaf68331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 8, Epochs: 1, Learning Rate: 0.0001, Accuracy: 90.55%\n",
            "Batch Size: 8, Epochs: 1, Learning Rate: 0.001, Accuracy: 90.09%\n",
            "Batch Size: 8, Epochs: 1, Learning Rate: 0.01, Accuracy: 11.35%\n",
            "Batch Size: 8, Epochs: 5, Learning Rate: 0.0001, Accuracy: 93.07%\n",
            "Batch Size: 8, Epochs: 5, Learning Rate: 0.001, Accuracy: 93.66%\n",
            "Batch Size: 8, Epochs: 5, Learning Rate: 0.01, Accuracy: 11.35%\n",
            "Batch Size: 8, Epochs: 10, Learning Rate: 0.0001, Accuracy: 95.46%\n",
            "Batch Size: 8, Epochs: 10, Learning Rate: 0.001, Accuracy: 92.83%\n",
            "Batch Size: 8, Epochs: 10, Learning Rate: 0.01, Accuracy: 9.82%\n",
            "Batch Size: 8, Epochs: 20, Learning Rate: 0.0001, Accuracy: 96.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Add Convolutional and Pooling Layers:"
      ],
      "metadata": {
        "id": "FMwiw6GD6aCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Update MLP class to include convolution and pooling layers\n",
        "class ConvolutionalMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalMLP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 1, kernel_size=2, stride=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.mlp = MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model and other parameters\n",
        "conv_mlp_model = ConvolutionalMLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(conv_mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and evaluation loop (similar to the previous one)\n",
        "# Make sure to use the correct dataloaders and adjust input sizes accordingly\n"
      ],
      "metadata": {
        "id": "lzCOlb8l6a-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Extend with Second Convolution-Pooling Layer:"
      ],
      "metadata": {
        "id": "RE7renQc6gqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConvolutionalMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DoubleConvolutionalMLP, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=2, stride=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(10, 10, kernel_size=2, stride=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.mlp = MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.mlp(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate model and other parameters\n",
        "double_conv_mlp_model = DoubleConvolutionalMLP()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(double_conv_mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and evaluation loop (similar to the previous one)\n",
        "# Make sure to use the correct dataloaders and adjust input sizes accordingly\n"
      ],
      "metadata": {
        "id": "4K_LBXGP6kXW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}